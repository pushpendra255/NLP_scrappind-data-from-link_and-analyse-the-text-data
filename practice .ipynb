{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c5200a-4579-4578-9888-2710483f05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # for data preprocessing and read csv files\n",
    "import numpy as np  # for numerical computation and manipulation \n",
    "\n",
    "from bs4 import BeautifulSoup   # Importing BeautifulSoup from the bs4 library for parsing HTML and XML documents.\n",
    "import requests        # Importing the requests library for making HTTP requests to retrieve web content.\n",
    "\n",
    "import os       # Importing the os library for interacting with the operating system, such as file and directory operations.\n",
    "import nltk     # Importing the nltk library Natural Language Toolkit, a powerful tool for working with human language data (text) in Python.\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize     # Importing RegexpTokenizer from nltk.tokenize to tokenize text using regular expressions.\n",
    "                                    # This is useful for creating custom tokenization patterns, such as splitting text by specific characters or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1060c6ac-ffc1-44e5-a8bf-7a46598e6905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>bctech2153</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>bctech2154</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>bctech2155</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>bctech2156</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>bctech2157</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         URL_ID                                                URL\n",
       "0    bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1    bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       "2    bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       "3    bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       "4    bctech2015  https://insights.blackcoffer.com/streamlined-t...\n",
       "..          ...                                                ...\n",
       "142  bctech2153  https://insights.blackcoffer.com/population-an...\n",
       "143  bctech2154  https://insights.blackcoffer.com/google-lsa-ap...\n",
       "144  bctech2155  https://insights.blackcoffer.com/healthcare-da...\n",
       "145  bctech2156  https://insights.blackcoffer.com/budget-sales-...\n",
       "146  bctech2157  https://insights.blackcoffer.com/amazon-buy-bo...\n",
       "\n",
       "[147 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Google Sheets link\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1D7QkDHxUSKnQhR--q0BAwKMxQlUyoJTQ/edit?usp=drive_link&ouid=115129819942252505059&rtpof=true&sd=true\"\n",
    "\n",
    "# Convert it to a CSV export link\n",
    "csv_export_url = sheet_url.replace('/edit?usp=drive_link&ouid=115129819942252505059&rtpof=true&sd=true', '/export?format=csv')\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_export_url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190f03c5-977d-4e8a-95b8-e67864a7e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "    title = soup.find(\"h1\").text.strip()\n",
    "    content = [i.text.strip() for i in soup.find_all(\"p\",class_ = None)]\n",
    "    return title, content\n",
    "\n",
    "\n",
    "# # notes -------------------------------------------------------------------- just uper wale ka -----\n",
    "\n",
    "# - parsing the HTML Content:\n",
    "# soup = BeautifulSoup(page.text, 'html.parser'):\n",
    "\n",
    "# This line uses BeautifulSoup to parse the HTML content of the page.\n",
    "# page.text contains the raw HTML of the page.\n",
    "# 'html.parser' is specified as the parser, which is a built-in parser in Python. It helps in navigating and manipulating HTML data.\n",
    "# - Why 'html.parser'?:\n",
    "# The html.parser tells BeautifulSoup how to parse the raw HTML text. There are other parsers available, like lxml or html5lib, but html.parser is a simple and effective choice for basic tasks.\n",
    "\n",
    "# - Extracting the Title:\n",
    "# title = soup.find('h1').text.strip():\n",
    "\n",
    "# This line finds the first <h1> tag in the HTML document.\n",
    "# The text attribute extracts the text content inside the <h1> tag.\n",
    "# .strip() is used to remove any leading or trailing whitespace from the text.\n",
    "# - Why <h1>?:\n",
    "# The <h1> tag is typically used to define the main heading or title of a web page. By extracting the text inside this tag, you often get the primary title of the page.\n",
    "\n",
    "# - Extracting the Paragraphs-\n",
    "# content = [i.text.strip() for i in soup.find_all('p', class_=None)]:\n",
    "\n",
    "# This line finds all <p> tags in the HTML that do not have a class attribute.\n",
    "# The find_all('p', class_=None) function returns a list of all <p> elements matching the criteria.\n",
    "# The for loop iterates over these elements, extracts the text with i.text.strip(), and stores them in the list content.\n",
    "# - Why <p> and class_=None?:\n",
    "# <p> tags generally contain paragraphs of text, which are key parts of web content.\n",
    "# Specifying class_=None ensures that only paragraphs without a specific class are selected, potentially filtering out less relevant content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ee1b5-e7dc-43df-8036-93958e4d4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows(): # For Loop with iterrows()\n",
    "    url = row[\"URL\"]   #Extracting URL and URL_ID\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    \n",
    "    # Print the URL ID\n",
    "    print(url_id)\n",
    "\n",
    "    \n",
    "    # Extract title and text from the URL\n",
    "    title, text = extract(url)\n",
    "\n",
    "    \n",
    "    # Check if the URL was reachable \n",
    "    if title is None:\n",
    "        print(f\"{j} is not reachable\")    #formatted string literal, commonly known as an f-string. F-strings \n",
    "        continue\n",
    "        file = f\"{j}.txt\"    # agr title none hai to not reachable print hoga , oor agr snone nahi hooga to continue rakhega....file ka naam dega....Create the filename based on the URL ID -       \n",
    "\n",
    "\n",
    "    # Write the title and text to the file - file ke ander title or text -\n",
    "    try:\n",
    "        with open(file, 'w+', encoding='utf-8') as f:         # write mode ('w+')\n",
    "            f.write(title + '\\n')        # Writing the Title -This writes the title of the page to the file, followed by a newline ('\\n') to separate it from the text.\n",
    "           \n",
    "            for line in text:       #  Writing Each Line of Text\n",
    "                f.write(line + '\\n')    # Purpose: This loop writes each paragraph of text (each item in the text list) to the file, followed by a newline\n",
    "                \n",
    "    except IOError as e:    # Exception Handling\n",
    "        print(f\"Failed to write file {file}: {e}\")   # ye error message dega ...\n",
    "\n",
    "\n",
    "# The term utf-8 refers to a character encoding that is widely used in computing ythe text in a unique no. for every characters. It stands for \"8-bit Unicode Transformation Format\"\n",
    "\n",
    "#  try is used to test a block of code for errors.\n",
    "# If an error occurs, the code inside the except block runs instead of crashing the program.\n",
    "# In your case, it's used to safely handle potential errors when writing to a file, ensuring the program can continue or provide a meaningful error message if something goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6493cf-be59-4b61-b63a-44c63e122ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL_ID: bctech2011\n",
      "Processing URL_ID: bctech2012\n",
      "Processing URL_ID: bctech2013\n",
      "Processing URL_ID: bctech2014\n",
      "Processing URL_ID: bctech2015\n",
      "Processing URL_ID: bctech2016\n",
      "Processing URL_ID: bctech2017\n",
      "Processing URL_ID: bctech2018\n",
      "Processing URL_ID: bctech2019\n",
      "Processing URL_ID: bctech2020\n",
      "Processing URL_ID: bctech2021\n",
      "Processing URL_ID: bctech2022\n",
      "Processing URL_ID: bctech2023\n",
      "Processing URL_ID: bctech2024\n",
      "Processing URL_ID: bctech2025\n",
      "Processing URL_ID: bctech2026\n",
      "Processing URL_ID: bctech2027\n",
      "Processing URL_ID: bctech2028\n",
      "Processing URL_ID: bctech2029\n",
      "Processing URL_ID: bctech2030\n",
      "Processing URL_ID: bctech2031\n",
      "Processing URL_ID: bctech2032\n",
      "Processing URL_ID: bctech2033\n",
      "Processing URL_ID: bctech2034\n",
      "Processing URL_ID: bctech2035\n",
      "Processing URL_ID: bctech2036\n",
      "Processing URL_ID: bctech2037\n",
      "Processing URL_ID: bctech2038\n",
      "Processing URL_ID: bctech2039\n",
      "Processing URL_ID: bctech2040\n",
      "Processing URL_ID: bctech2041\n",
      "Processing URL_ID: bctech2042\n",
      "Processing URL_ID: bctech2043\n",
      "Processing URL_ID: bctech2044\n",
      "Processing URL_ID: bctech2045\n",
      "Processing URL_ID: bctech2046\n",
      "Processing URL_ID: bctech2047\n",
      "Processing URL_ID: bctech2048\n",
      "Processing URL_ID: bctech2049\n",
      "Processing URL_ID: bctech2050\n",
      "Processing URL_ID: bctech2051\n",
      "Processing URL_ID: bctech2052\n",
      "Processing URL_ID: bctech2053\n",
      "Processing URL_ID: bctech2054\n",
      "Processing URL_ID: bctech2055\n",
      "Processing URL_ID: bctech2056\n",
      "Processing URL_ID: bctech2057\n",
      "Processing URL_ID: bctech2058\n",
      "Processing URL_ID: bctech2059\n",
      "Processing URL_ID: bctech2060\n",
      "Processing URL_ID: bctech2061\n",
      "Processing URL_ID: bctech2062\n",
      "Processing URL_ID: bctech2063\n",
      "Processing URL_ID: bctech2064\n",
      "Processing URL_ID: bctech2065\n",
      "Processing URL_ID: bctech2066\n",
      "Processing URL_ID: bctech2067\n",
      "Processing URL_ID: bctech2068\n",
      "Processing URL_ID: bctech2069\n",
      "Processing URL_ID: bctech2070\n",
      "Processing URL_ID: bctech2071\n",
      "Processing URL_ID: bctech2072\n",
      "Processing URL_ID: bctech2073\n",
      "Processing URL_ID: bctech2074\n",
      "Processing URL_ID: bctech2075\n",
      "Processing URL_ID: bctech2076\n",
      "Processing URL_ID: bctech2077\n",
      "Processing URL_ID: bctech2078\n",
      "Processing URL_ID: bctech2079\n",
      "Processing URL_ID: bctech2080\n",
      "Processing URL_ID: bctech2081\n",
      "Processing URL_ID: bctech2082\n",
      "Processing URL_ID: bctech2083\n",
      "Processing URL_ID: bctech2084\n",
      "Processing URL_ID: bctech2085\n",
      "Processing URL_ID: bctech2086\n",
      "Processing URL_ID: bctech2087\n",
      "Processing URL_ID: bctech2088\n",
      "Processing URL_ID: bctech2089\n",
      "Processing URL_ID: bctech2090\n",
      "Processing URL_ID: bctech2091\n",
      "Processing URL_ID: bctech2092\n",
      "Processing URL_ID: bctech2093\n",
      "Processing URL_ID: bctech2094\n",
      "Processing URL_ID: bctech2095\n",
      "Processing URL_ID: bctech2096\n",
      "Processing URL_ID: bctech2097\n",
      "Processing URL_ID: bctech2098\n",
      "Processing URL_ID: bctech2099\n",
      "Processing URL_ID: bctech2100\n",
      "Processing URL_ID: bctech2101\n",
      "Processing URL_ID: bctech2102\n",
      "Processing URL_ID: bctech2103\n",
      "Processing URL_ID: bctech2104\n",
      "Processing URL_ID: bctech2105\n",
      "Processing URL_ID: bctech2106\n",
      "Processing URL_ID: bctech2107\n",
      "Processing URL_ID: bctech2108\n",
      "Processing URL_ID: bctech2109\n",
      "Processing URL_ID: bctech2110\n",
      "Processing URL_ID: bctech2111\n",
      "Processing URL_ID: bctech2112\n",
      "Processing URL_ID: bctech2113\n",
      "Processing URL_ID: bctech2114\n",
      "Processing URL_ID: bctech2115\n",
      "Processing URL_ID: bctech2116\n",
      "Processing URL_ID: bctech2117\n",
      "Processing URL_ID: bctech2118\n",
      "Processing URL_ID: bctech2119\n",
      "Processing URL_ID: bctech2120\n",
      "Processing URL_ID: bctech2121\n",
      "Processing URL_ID: bctech2122\n",
      "Processing URL_ID: bctech2123\n",
      "Processing URL_ID: bctech2124\n",
      "Processing URL_ID: bctech2125\n",
      "Processing URL_ID: bctech2126\n",
      "Processing URL_ID: bctech2127\n",
      "Processing URL_ID: bctech2128\n",
      "Processing URL_ID: bctech2129\n",
      "Processing URL_ID: bctech2130\n",
      "Processing URL_ID: bctech2131\n",
      "Processing URL_ID: bctech2132\n",
      "Processing URL_ID: bctech2133\n",
      "Processing URL_ID: bctech2134\n",
      "Processing URL_ID: bctech2135\n",
      "Processing URL_ID: bctech2136\n",
      "Processing URL_ID: bctech2137\n",
      "Processing URL_ID: bctech2138\n",
      "Processing URL_ID: bctech2139\n",
      "Processing URL_ID: bctech2140\n",
      "Processing URL_ID: bctech2141\n",
      "Processing URL_ID: bctech2142\n",
      "Processing URL_ID: bctech2143\n",
      "Processing URL_ID: bctech2144\n",
      "Processing URL_ID: bctech2145\n",
      "Processing URL_ID: bctech2146\n",
      "Processing URL_ID: bctech2147\n",
      "Processing URL_ID: bctech2148\n",
      "Processing URL_ID: bctech2149\n",
      "Processing URL_ID: bctech2150\n",
      "Processing URL_ID: bctech2151\n",
      "Processing URL_ID: bctech2152\n",
      "Processing URL_ID: bctech2153\n",
      "Processing URL_ID: bctech2154\n",
      "Processing URL_ID: bctech2155\n",
      "Processing URL_ID: bctech2156\n",
      "Processing URL_ID: bctech2157\n"
     ]
    }
   ],
   "source": [
    "# Initialize NLTK resources\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "pronouns = {\"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"them\", \"us\", \"him\", \"her\", \"his\", \"hers\", \"its\", \"theirs\", \"our\", \"your\"}\n",
    "\n",
    "# Directory path where the text files are located\n",
    "directory_path = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\Blackcoffer_internship'                  \n",
    "\n",
    "# Define paths for the positive and negative words files\n",
    "positive_words_path = os.path.join(directory_path, 'positive-words.txt')\n",
    "negative_words_path = os.path.join(directory_path, 'negative-words.txt')\n",
    "\n",
    "# Global Variables: Variables declared outside functions that can be accessed anywhere in the code......isi liye function me ham kahi bhi dubara nahi diye hai file ka path ...ek baar upar de do syster automaticaly...sab kar lega....\n",
    "\n",
    "\n",
    "def read_words(file_path): # in place of file_path we can give any sting.\n",
    "    \"\"\"Helper function to read and tokenize words from a file.\"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    return set(tokenizer.tokenize(text.lower()))\n",
    "\n",
    "def positive_score(file):  # in place of file we can give any sting.\n",
    "    \"\"\"Calculate the positive score for the given file.\"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print(f\"File not found: {file}\")\n",
    "        return 0\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    positive_words = read_words(positive_words_path)\n",
    "    \n",
    "    count = sum(1 for word in words if word in positive_words)\n",
    "    return count\n",
    "\n",
    "def negative_score(file):\n",
    "    \"\"\"Calculate the negative score for the given file.\"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print(f\"File not found: {file}\")\n",
    "        return 0\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    negative_words = read_words(negative_words_path)\n",
    "    \n",
    "    count = sum(-1 for word in words if word in negative_words)\n",
    "    return count\n",
    "\n",
    "def polarity(file):\n",
    "    \"\"\"Calculate the polarity score for the given file.\"\"\"\n",
    "    pos_score = positive_score(file)\n",
    "    neg_score = negative_score(file)\n",
    "    return (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
    "\n",
    "def subjectivity(file):\n",
    "    \"\"\"Calculate the subjectivity score for the given file.\"\"\"\n",
    "    pos_score = positive_score(file)\n",
    "    neg_score = negative_score(file)\n",
    "    total_words = len(tokenizer.tokenize(open(file, 'r', encoding='utf-8', errors='ignore').read().lower()))\n",
    "    return (pos_score + neg_score) / (total_words + 0.000001)\n",
    "\n",
    "def calculate_metrics(file_id):\n",
    "    \"\"\"Calculate all metrics for a given file ID.\"\"\"\n",
    "    file_path = os.path.join(directory_path, file_id + '.txt')  # Ensure .txt extension\n",
    "    \n",
    "    # Initialize metrics\n",
    "    pos_score = positive_score(file_path)\n",
    "    neg_score = negative_score(file_path)\n",
    "    pol_score = polarity(file_path)\n",
    "    subj_score = subjectivity(file_path)\n",
    "    \n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Tokenize words and sentences\n",
    "    words = tokenizer.tokenize(text)\n",
    "    filtered_words = [w for w in words if w.lower() not in stop_words]\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Calculate average sentence length\n",
    "    sentence_count = len(sentences)\n",
    "    word_count = len(filtered_words)\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
    "    \n",
    "    # Calculate complex words\n",
    "    vowels = 'aeiouy'\n",
    "    count_complex_words = 0\n",
    "    for word in filtered_words:\n",
    "        word = word.lower()\n",
    "        count = 0\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for i in range(1, len(word)):\n",
    "            if word[i] in vowels and word[i - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"e\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        if count > 2:\n",
    "            count_complex_words += 1\n",
    "    \n",
    "    percentage_complex_words = count_complex_words / len(filtered_words) if len(filtered_words) else 0\n",
    "    \n",
    "    # Calculate FOG index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words) if avg_sentence_length else 0\n",
    "    \n",
    "    # Calculate average number of words per sentence\n",
    "    total_words = len(words)\n",
    "    avg_words_per_sentence = total_words / sentence_count if sentence_count else 0\n",
    "    \n",
    "    # Calculate syllables per word\n",
    "    total_syllables = 0\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        count = 0\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for i in range(1, len(word)):\n",
    "            if word[i] in vowels and word[i - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"e\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        total_syllables += count\n",
    "    \n",
    "    syllables_per_word = total_syllables / len(words) if words else 0\n",
    "    \n",
    "    # Calculate personal pronouns count\n",
    "    personal_pronouns_count = sum(1 for word in words if word.lower() in pronouns)\n",
    "    \n",
    "    # Calculate average word length\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    avg_word_length = total_length / len(words) if words else 0\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    return {\n",
    "        'POSITIVE SCORE': pos_score,\n",
    "        'NEGATIVE SCORE': neg_score,\n",
    "        'POLARITY SCORE': pol_score,\n",
    "        'SUBJECTIVITY SCORE': subj_score,\n",
    "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
    "        'FOG INDEX': fog_index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
    "        'COMPLEX WORD COUNT': count_complex_words,\n",
    "        'WORD COUNT': word_count,\n",
    "        'SYLLABLE PER WORD': syllables_per_word,\n",
    "        'PERSONAL PRONOUNS': personal_pronouns_count,\n",
    "        'AVG WORD LENGTH': avg_word_length\n",
    "    }\n",
    "\n",
    "# Initialize an empty DataFrame for output\n",
    "df_output = pd.DataFrame(columns=[\n",
    "    'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "    'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
    "    'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
    "    'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "])\n",
    "\n",
    "def calculate_set(file_id, index):\n",
    "    metrics = calculate_metrics(file_id)  # Get metrics for the file\n",
    "    \n",
    "    # Store the calculated values in the df_output DataFrame\n",
    "    df_output.loc[index, 'URL_ID'] = file_id\n",
    "    df_output.loc[index, 'URL'] = df.loc[index, 'URL']\n",
    "    df_output.loc[index, 'POSITIVE SCORE'] = metrics['POSITIVE SCORE']\n",
    "    df_output.loc[index, 'NEGATIVE SCORE'] = metrics['NEGATIVE SCORE']\n",
    "    df_output.loc[index, 'POLARITY SCORE'] = metrics['POLARITY SCORE']\n",
    "    df_output.loc[index, 'SUBJECTIVITY SCORE'] = metrics['SUBJECTIVITY SCORE']\n",
    "    df_output.loc[index, 'AVG SENTENCE LENGTH'] = metrics['AVG SENTENCE LENGTH']\n",
    "    df_output.loc[index, 'PERCENTAGE OF COMPLEX WORDS'] = metrics['PERCENTAGE OF COMPLEX WORDS']\n",
    "    df_output.loc[index, 'FOG INDEX'] = metrics['FOG INDEX']\n",
    "    df_output.loc[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = metrics['AVG NUMBER OF WORDS PER SENTENCE']\n",
    "    df_output.loc[index, 'COMPLEX WORD COUNT'] = metrics['COMPLEX WORD COUNT']\n",
    "    df_output.loc[index, 'WORD COUNT'] = metrics['WORD COUNT']\n",
    "    df_output.loc[index, 'SYLLABLE PER WORD'] = metrics['SYLLABLE PER WORD']\n",
    "    df_output.loc[index, 'PERSONAL PRONOUNS'] = metrics['PERSONAL PRONOUNS']\n",
    "    df_output.loc[index, 'AVG WORD LENGTH'] = metrics['AVG WORD LENGTH']\n",
    "\n",
    "# Process each file\n",
    "for index, row in df.iterrows():\n",
    "    file_id = row.get('URL_ID', 'Default_ID')  # Use .get() with a default value if key might be missing\n",
    "    print(f\"Processing URL_ID: {file_id}\")\n",
    "    try:\n",
    "        calculate_set(file_id, index)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(df_output)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "#df_output.to_excel('Output.xlsx', index=False)  # Save as Excel file\n",
    "\n",
    "# Optionally, save as CSV file\n",
    "df_output.to_csv('Output.csv', index=False)  # Save as CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6acf98-d4c2-4db2-9a23-ba8d5e083938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "      <td>159</td>\n",
       "      <td>-26</td>\n",
       "      <td>1.390977</td>\n",
       "      <td>0.167506</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.421429</td>\n",
       "      <td>11.368571</td>\n",
       "      <td>39.7</td>\n",
       "      <td>236</td>\n",
       "      <td>560</td>\n",
       "      <td>1.982368</td>\n",
       "      <td>6</td>\n",
       "      <td>5.647355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "      <td>8</td>\n",
       "      <td>-3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.05618</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>9.727778</td>\n",
       "      <td>29.666667</td>\n",
       "      <td>23</td>\n",
       "      <td>72</td>\n",
       "      <td>1.921348</td>\n",
       "      <td>1</td>\n",
       "      <td>6.067416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "      <td>11</td>\n",
       "      <td>-3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>9.75</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>27</td>\n",
       "      <td>72</td>\n",
       "      <td>1.934066</td>\n",
       "      <td>3</td>\n",
       "      <td>5.989011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "      <td>9</td>\n",
       "      <td>-4</td>\n",
       "      <td>2.599999</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>9.75</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>27</td>\n",
       "      <td>72</td>\n",
       "      <td>1.967033</td>\n",
       "      <td>2</td>\n",
       "      <td>6.021978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "      <td>9</td>\n",
       "      <td>-3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>9.886758</td>\n",
       "      <td>30.0</td>\n",
       "      <td>28</td>\n",
       "      <td>73</td>\n",
       "      <td>1.988889</td>\n",
       "      <td>1</td>\n",
       "      <td>6.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>bctech2153</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "      <td>138</td>\n",
       "      <td>-40</td>\n",
       "      <td>1.816327</td>\n",
       "      <td>0.115159</td>\n",
       "      <td>17.323529</td>\n",
       "      <td>0.317487</td>\n",
       "      <td>7.056407</td>\n",
       "      <td>25.029412</td>\n",
       "      <td>187</td>\n",
       "      <td>589</td>\n",
       "      <td>1.836663</td>\n",
       "      <td>5</td>\n",
       "      <td>5.204465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>bctech2154</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "      <td>254</td>\n",
       "      <td>-57</td>\n",
       "      <td>1.57868</td>\n",
       "      <td>0.148679</td>\n",
       "      <td>13.609375</td>\n",
       "      <td>0.299656</td>\n",
       "      <td>5.563612</td>\n",
       "      <td>20.703125</td>\n",
       "      <td>261</td>\n",
       "      <td>871</td>\n",
       "      <td>1.792453</td>\n",
       "      <td>13</td>\n",
       "      <td>5.273208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>bctech2155</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "      <td>51</td>\n",
       "      <td>-17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>12.363636</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>5.018984</td>\n",
       "      <td>21.818182</td>\n",
       "      <td>25</td>\n",
       "      <td>136</td>\n",
       "      <td>1.525</td>\n",
       "      <td>11</td>\n",
       "      <td>4.5875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>bctech2156</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>6.55</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3.125</td>\n",
       "      <td>0</td>\n",
       "      <td>9.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>bctech2157</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>4.34749</td>\n",
       "      <td>16.571429</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "      <td>1.646552</td>\n",
       "      <td>2</td>\n",
       "      <td>4.818966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         URL_ID                                                URL  \\\n",
       "0    bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       "1    bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       "2    bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       "3    bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
       "4    bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
       "..          ...                                                ...   \n",
       "142  bctech2153  https://insights.blackcoffer.com/population-an...   \n",
       "143  bctech2154  https://insights.blackcoffer.com/google-lsa-ap...   \n",
       "144  bctech2155  https://insights.blackcoffer.com/healthcare-da...   \n",
       "145  bctech2156  https://insights.blackcoffer.com/budget-sales-...   \n",
       "146  bctech2157  https://insights.blackcoffer.com/amazon-buy-bo...   \n",
       "\n",
       "    POSITIVE SCORE NEGATIVE SCORE POLARITY SCORE SUBJECTIVITY SCORE  \\\n",
       "0              159            -26       1.390977           0.167506   \n",
       "1                8             -3            2.2            0.05618   \n",
       "2               11             -3           1.75           0.087912   \n",
       "3                9             -4       2.599999           0.054945   \n",
       "4                9             -3            2.0           0.066667   \n",
       "..             ...            ...            ...                ...   \n",
       "142            138            -40       1.816327           0.115159   \n",
       "143            254            -57        1.57868           0.148679   \n",
       "144             51            -17            2.0           0.141667   \n",
       "145              0              0            0.0                0.0   \n",
       "146             24              0            1.0           0.206897   \n",
       "\n",
       "    AVG SENTENCE LENGTH PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                  28.0                    0.421429  11.368571   \n",
       "1                  24.0                    0.319444   9.727778   \n",
       "2                  24.0                       0.375       9.75   \n",
       "3                  24.0                       0.375       9.75   \n",
       "4             24.333333                    0.383562   9.886758   \n",
       "..                  ...                         ...        ...   \n",
       "142           17.323529                    0.317487   7.056407   \n",
       "143           13.609375                    0.299656   5.563612   \n",
       "144           12.363636                    0.183824   5.018984   \n",
       "145                16.0                       0.375       6.55   \n",
       "146           10.571429                    0.297297    4.34749   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE COMPLEX WORD COUNT WORD COUNT  \\\n",
       "0                               39.7                236        560   \n",
       "1                          29.666667                 23         72   \n",
       "2                          30.333333                 27         72   \n",
       "3                          30.333333                 27         72   \n",
       "4                               30.0                 28         73   \n",
       "..                               ...                ...        ...   \n",
       "142                        25.029412                187        589   \n",
       "143                        20.703125                261        871   \n",
       "144                        21.818182                 25        136   \n",
       "145                             16.0                  6         16   \n",
       "146                        16.571429                 22         74   \n",
       "\n",
       "    SYLLABLE PER WORD PERSONAL PRONOUNS AVG WORD LENGTH  \n",
       "0            1.982368                 6        5.647355  \n",
       "1            1.921348                 1        6.067416  \n",
       "2            1.934066                 3        5.989011  \n",
       "3            1.967033                 2        6.021978  \n",
       "4            1.988889                 1        6.166667  \n",
       "..                ...               ...             ...  \n",
       "142          1.836663                 5        5.204465  \n",
       "143          1.792453                13        5.273208  \n",
       "144             1.525                11          4.5875  \n",
       "145             3.125                 0          9.1875  \n",
       "146          1.646552                 2        4.818966  \n",
       "\n",
       "[147 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086f1a2-7106-44c7-97ce-c22b45fd9dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
